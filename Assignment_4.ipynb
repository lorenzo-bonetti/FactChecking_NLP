{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgqFYgTo4pCr"
      },
      "source": [
        "# Dataset pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsLB1Ew4W3oF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIVdEZrNW7d2"
      },
      "source": [
        "import os\r\n",
        "import requests\r\n",
        "import zipfile\r\n",
        "\r\n",
        "import re\r\n",
        "from functools import reduce\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import gensim\r\n",
        "import gensim.downloader as gloader\r\n",
        "\r\n",
        "import scipy\r\n",
        "import gc\r\n",
        "import time\r\n",
        "\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1RPTTLWWePX"
      },
      "source": [
        "from keras.layers import Masking\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Reshape\r\n",
        "from keras.layers import Masking\r\n",
        "from keras.layers import GlobalAveragePooling1D\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "from keras.layers import RepeatVector\r\n",
        "from keras.layers import concatenate\r\n",
        "from keras.layers import Average\r\n",
        "from keras.layers import Add\r\n",
        "from keras.layers import Lambda\r\n",
        "from keras import Model\r\n",
        "from keras.utils import plot_model\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from scipy.spatial.distance import cosine "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSfpIZjAhX5G"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspxZcRjW0NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd4ce1f-630e-4de5-f3c3-5175e3c2f6af"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading FEVER data splits...\n",
            "Download completed!\n",
            "Extracting dataset...\n",
            "Extraction completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q2WIZitzfRZ",
        "outputId": "c0647800-8d20-4e00-8eef-92bb6684a947"
      },
      "source": [
        "cwd = os.getcwd()\n",
        "train_df = pd.read_csv(cwd + \"/dataset/train_pairs.csv\")\n",
        "val_df = pd.read_csv(cwd + \"/dataset/val_pairs.csv\")\n",
        "test_df = pd.read_csv(cwd + \"/dataset/test_pairs.csv\")\n",
        "\n",
        "#drop first column\n",
        "train_df = train_df.drop(train_df.columns[0], axis=1)\n",
        "val_df = val_df.drop(val_df.columns[0], axis=1)\n",
        "test_df = test_df.drop(test_df.columns[0], axis=1)\n",
        "\n",
        "#Text cleaning: removing tags from evidence\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;.:`\\-\\'\\\"]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z #+_\\|@,;.:`\\-\\'\\\"\\\\\\/]')\n",
        "REMOVE_SB_TAGS = re.compile('-LSB-(.*?)-RSB-')\n",
        "REMOVE_RB_TAGS = re.compile('-LRB-|-RRB-')\n",
        "TEXT_IN_PARS = re.compile('-LRB-(.*?)-RRB-')\n",
        "\n",
        "def replace_double_apix(text):\n",
        "  return text.replace(\"''\", '\"')\n",
        "\n",
        "def replace_special_characters(text):\n",
        "  return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def remove_SB_text(text):\n",
        "  return REMOVE_SB_TAGS.sub('', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text):\n",
        "  return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def handle_parentheses(text):\n",
        "  sentences = re.findall(TEXT_IN_PARS, text)\n",
        "  for sent in sentences: \n",
        "    if re.search(GOOD_SYMBOLS_RE, sent) is not None:\n",
        "      text = TEXT_IN_PARS.sub('', text, 1)\n",
        "    else:\n",
        "      text = REMOVE_RB_TAGS.sub('', text, 2)\n",
        "  return text\n",
        "    \n",
        "def strip_text(text):\n",
        "  return \" \".join(text.split())\n",
        "\n",
        "def lower_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def replace_genitivo(text):\n",
        "  return text.replace(\"'s\", \" 's\")\n",
        "\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          replace_double_apix,\n",
        "                          remove_SB_text,\n",
        "                          handle_parentheses,\n",
        "                          replace_special_characters,\n",
        "                          replace_genitivo,\n",
        "                          strip_text,\n",
        "                          lower_text\n",
        "                          ]\n",
        "\n",
        "def text_prepare(text, filter_methods=PREPROCESSING_PIPELINE):\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "def clean_evidence_texts(df):\n",
        "  df['Evidence'] = df['Evidence'].apply(lambda x: x.split('\\t')[1])\n",
        "  df['Evidence'] = df['Evidence'].apply(lambda txt: text_prepare(txt))\n",
        "  df['Evidence'] = df['Evidence'].apply(lambda x: x.split())\n",
        "\n",
        "def clean_claim_texts(df):\n",
        "  df['Claim'] = df['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "  df['Claim'] = df['Claim'].apply(lambda x: x.split())\n",
        "\n",
        "clean_evidence_texts(train_df)\n",
        "clean_evidence_texts(val_df)\n",
        "clean_evidence_texts(test_df)\n",
        "clean_claim_texts(train_df)\n",
        "clean_claim_texts(val_df)\n",
        "clean_claim_texts(test_df)\n",
        "\n",
        "print('Training set shape:', train_df.shape)\n",
        "print('Validation set shape:', val_df.shape)\n",
        "print('Test set shape:', test_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set shape: (121740, 4)\n",
            "Validation set shape: (7165, 4)\n",
            "Test set shape: (7189, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4_KpJBT46po"
      },
      "source": [
        "# Dataset conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVhGF5G6EO5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80885c69-367a-4901-bdfe-b270e7b34a53"
      },
      "source": [
        "embedding_dimension = 300\r\n",
        "download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\r\n",
        "try:\r\n",
        "  embedding_model = gloader.load(download_path)\r\n",
        "except ValueError as e:\r\n",
        "  print(\"Invalid embedding model name! Check the embedding dimension:\")\r\n",
        "  print(\"Glove: 50, 100, 200, 300\")\r\n",
        "  raise e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouAXW4vEYX8",
        "outputId": "2546912c-5622-44c0-8be6-f0024f2193fa"
      },
      "source": [
        "def build_vocabulary(corpus):\r\n",
        "\r\n",
        "  wordlist = []\r\n",
        "  for x in corpus:\r\n",
        "    wordlist.extend(x)\r\n",
        "  words = set(wordlist)\r\n",
        "  word_vocab = {}\r\n",
        "  inverse_word_vocab = {}\r\n",
        "  for i, word in enumerate(words):\r\n",
        "    word_vocab[i] = word\r\n",
        "    inverse_word_vocab[word] = i\r\n",
        "  \r\n",
        "  return word_vocab, inverse_word_vocab, words\r\n",
        "\r\n",
        "\r\n",
        "corpus = pd.concat([train_df['Claim'], train_df['Evidence'], val_df['Claim'], val_df['Evidence'], test_df['Claim'], test_df['Evidence']], ignore_index=True)\r\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(corpus)\r\n",
        "print(len(word_listing))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hc9QYNVIkiV",
        "outputId": "2d95577f-4cfe-43df-8a6a-e739e10aef29"
      },
      "source": [
        "def co_occurrence_count(corpus, idx_to_word, word_to_idx, window_size=1):\r\n",
        "\r\n",
        "    data = []\r\n",
        "    index_i = []\r\n",
        "    index_j = []\r\n",
        "\r\n",
        "    for _, words in corpus.iteritems():\r\n",
        "      for j, word in enumerate(words[::]):\r\n",
        "        start = max(0, j-window_size)\r\n",
        "        end = min(len(words), j + window_size+1)\r\n",
        "        sub_sentence = words[start:end]\r\n",
        "        for w in sub_sentence:\r\n",
        "          if word != w:\r\n",
        "            data.append(1.)\r\n",
        "            index_i.append(word_to_idx[word])\r\n",
        "            index_j.append(word_to_idx[w])\r\n",
        "            \r\n",
        "    co_occurrence = scipy.sparse.csr_matrix((data, (index_i, index_j)))\r\n",
        "\r\n",
        "    return co_occurrence\r\n",
        "\r\n",
        "window_size = 1\r\n",
        "\r\n",
        "# Clean RAM before re-running this code snippet to avoid session crash\r\n",
        "if 'co_occurrence_matrix' in globals():\r\n",
        "    del co_occurrence_matrix\r\n",
        "    gc.collect()\r\n",
        "    time.sleep(10.)\r\n",
        "\r\n",
        "\r\n",
        "print(\"Building co-occurrence count matrix... (it may take a while...)\")\r\n",
        "co_occurrence_matrix = co_occurrence_count(corpus, idx_to_word, word_to_idx, window_size)\r\n",
        "\r\n",
        "print(\"Building completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building co-occurrence count matrix... (it may take a while...)\n",
            "Building completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGpShbaSJWo5",
        "outputId": "2acbc70d-4633-4146-f2c5-96e30a81d77f"
      },
      "source": [
        "def check_OOV_terms(embedding_model, word_listing):\r\n",
        "  OOV = [word for word in word_listing if word not in embedding_model.vocab]\r\n",
        "  return OOV\r\n",
        "\r\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\r\n",
        "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_listing)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total OOV terms: 2342 (6.90%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7xdmhKPPSIg",
        "outputId": "abadbc53-7c86-4fad-854a-d507aceb958e"
      },
      "source": [
        "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, co_occurrence_matrix):\r\n",
        "   \r\n",
        "    embedding_matrix = np.ndarray((len(word_to_idx), embedding_dimension))\r\n",
        "\r\n",
        "    for w in word_to_idx:\r\n",
        "      if w in embedding_model.vocab:\r\n",
        "        embedding_matrix[word_to_idx[w], :] = embedding_model.get_vector(w)\r\n",
        "      else:\r\n",
        "        occurrences = co_occurrence_matrix[word_to_idx[w]]\r\n",
        "\r\n",
        "        close_words = []\r\n",
        "        for i, d in zip(occurrences.indices, occurrences.data):\r\n",
        "        \r\n",
        "          if idx_to_word[i] in embedding_model.vocab:\r\n",
        "            close_words.append(embedding_model.get_vector(idx_to_word[i]) * d)\r\n",
        "        if len(close_words) == 0:\r\n",
        "          embedding_matrix[word_to_idx[w], :] = np.random.rand(1, embedding_dimension)\r\n",
        "        else:\r\n",
        "          embedding_matrix[word_to_idx[w], :] = np.average(close_words)\r\n",
        "\r\n",
        "    return embedding_matrix \r\n",
        "  \r\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, co_occurrence_matrix)\r\n",
        "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (33939, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKbKcIUNpOyG",
        "outputId": "1e2a1852-ad63-432c-dcb4-a3ff6c0d6479"
      },
      "source": [
        "MAX_SENTENCE_LENGTH = max(len(x) for x in corpus)\r\n",
        "print(\"Longest sentence: {} words\".format(MAX_SENTENCE_LENGTH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest sentence: 121 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV6ILvN60pz7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "a2d65565-11cf-45a9-a6ee-09995694b137"
      },
      "source": [
        "def embed_sentence(embedding_matrix, sentence, word_to_idx):\r\n",
        "  embedded = []\r\n",
        "  for w in sentence:\r\n",
        "    embedded.append(embedding_matrix[word_to_idx[w]])\r\n",
        "  return embedded\r\n",
        "\r\n",
        "def embed_sentences(df, embedding_matrix, word_to_idx):\r\n",
        "  df['Embedded claim'] = [embed_sentence(embedding_matrix, sentence, word_to_idx) for sentence in df['Claim']]\r\n",
        "  df['Embedded evidence'] = [embed_sentence(embedding_matrix, sentence, word_to_idx) for sentence in df['Evidence']]\r\n",
        "  \r\n",
        "  label_encoder = LabelEncoder()\r\n",
        "  label_encoder.fit(df['Label'])\r\n",
        "  \r\n",
        "  df['Embedded label'] =label_encoder.transform(df['Label'])\r\n",
        "\r\n",
        "\r\n",
        "def dataframe_generator(df):\r\n",
        "  generator = (([np.array(pad_sequences([df['Embedded claim'][i]], maxlen=MAX_SENTENCE_LENGTH, padding='post', dtype='float32')),\r\n",
        "                np.array(pad_sequences([df['Embedded evidence'][i]], maxlen=MAX_SENTENCE_LENGTH, padding='post', dtype='float32'))],\r\n",
        "                np.array(df['Embedded label'][i]).astype('float32').reshape((-1,1))) for i in range(len(df)))\r\n",
        "  return generator\r\n",
        "\r\n",
        "embed_sentences(train_df, embedding_matrix, word_to_idx)\r\n",
        "embed_sentences(val_df, embedding_matrix, word_to_idx)\r\n",
        "embed_sentences(test_df, embedding_matrix, word_to_idx)\r\n",
        "\r\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Claim</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "      <th>Embedded claim</th>\n",
              "      <th>Embedded evidence</th>\n",
              "      <th>Embedded label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[chris, hemsworth, appeared, in, a, perfect, g...</td>\n",
              "      <td>[hemsworth, has, also, appeared, in, the, scie...</td>\n",
              "      <td>3</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[[0.43830999732017517, -0.22437000274658203, -...</td>\n",
              "      <td>[[0.2014700025320053, -0.5208799839019775, 0.2...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[roald, dahl, is, a, writer]</td>\n",
              "      <td>[roald, dahl, 13, september, 1916, 23, novembe...</td>\n",
              "      <td>7</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[[0.4275299906730652, 0.20163999497890472, -0....</td>\n",
              "      <td>[[0.4275299906730652, 0.20163999497890472, -0....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[roald, dahl, is, a, governor]</td>\n",
              "      <td>[roald, dahl, 13, september, 1916, 23, novembe...</td>\n",
              "      <td>8</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[[0.4275299906730652, 0.20163999497890472, -0....</td>\n",
              "      <td>[[0.4275299906730652, 0.20163999497890472, -0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[ireland, has, relatively, low, lying, mountains]</td>\n",
              "      <td>[the, island, s, geography, comprises, relativ...</td>\n",
              "      <td>9</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[[0.5416100025177002, 0.2409999966621399, 0.01...</td>\n",
              "      <td>[[0.046560000628232956, 0.21318000555038452, -...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ireland, does, not, have, relatively, low, ly...</td>\n",
              "      <td>[the, island, s, geography, comprises, relativ...</td>\n",
              "      <td>10</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[[0.5416100025177002, 0.2409999966621399, 0.01...</td>\n",
              "      <td>[[0.046560000628232956, 0.21318000555038452, -...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Claim  ... Embedded label\n",
              "0  [chris, hemsworth, appeared, in, a, perfect, g...  ...              1\n",
              "1                       [roald, dahl, is, a, writer]  ...              1\n",
              "2                     [roald, dahl, is, a, governor]  ...              0\n",
              "3  [ireland, has, relatively, low, lying, mountains]  ...              1\n",
              "4  [ireland, does, not, have, relatively, low, ly...  ...              0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKdMC0hp5D4F"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4qq5resY7Xm"
      },
      "source": [
        "from keras import backend as K\r\n",
        "\r\n",
        "def cosine_distance(vests):\r\n",
        "    x, y = vests\r\n",
        "    x = K.l2_normalize(x, axis=-1)\r\n",
        "    y = K.l2_normalize(y, axis=-1)\r\n",
        "    return -K.mean(x * y, axis=-1, keepdims=True)\r\n",
        "\r\n",
        "def cos_dist_output_shape(shapes):\r\n",
        "    shape1, shape2 = shapes\r\n",
        "    return (shape1[0],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u36sfNUO42D"
      },
      "source": [
        "## simpleLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTmATcH0FT9F"
      },
      "source": [
        "def simple_LSTM_encoder(input_shape, merging_technique, add_similarity):\r\n",
        "  input_claim = Input(shape=input_shape)\r\n",
        "  mask_claim = Masking(input_shape=input_shape, mask_value=0.0)(input_claim)\r\n",
        "  drop_claim = Dropout(0.2)(mask_claim)\r\n",
        "  lstm_claim = Bidirectional(LSTM(64, return_sequences=False, dropout=0.4))(drop_claim)\r\n",
        "  \r\n",
        "  input_evidence = Input(shape=input_shape)\r\n",
        "  mask_evidence = Masking(input_shape=input_shape, mask_value=0.0)(input_evidence)\r\n",
        "  drop_evidence = Dropout(0.2)(mask_evidence)\r\n",
        "  lstm_evidence = Bidirectional(LSTM(64, return_sequences=False, dropout=0.4))(drop_evidence)\r\n",
        "\r\n",
        "  if merging_technique == 'concatenation':\r\n",
        "    merge = concatenate([lstm_claim, lstm_evidence])\r\n",
        "  elif merging_technique == 'sum':\r\n",
        "    merge = Add()([lstm_claim, lstm_evidence])\r\n",
        "  elif merging_technique == 'average':\r\n",
        "    merge = Average()([lstm_claim, lstm_evidence])\r\n",
        "\r\n",
        "  if add_similarity:\r\n",
        "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([lstm_claim, lstm_evidence])\r\n",
        "    merge = concatenate([merge, distance])\r\n",
        "\r\n",
        "  classificator = Dense(512, activation='relu')(merge)\r\n",
        "  classificator = Dropout(0.2)(classificator)\r\n",
        "  classificator = Dense(1, activation='sigmoid')(classificator)\r\n",
        "  \r\n",
        "  opt = Adam(learning_rate=0.001)\r\n",
        "  \r\n",
        "  model = Model(inputs=[input_claim, input_evidence], outputs=classificator)\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16HrV39QO_da"
      },
      "source": [
        "## averageLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ua2wtvfPDNP"
      },
      "source": [
        "def average_LSTM_encoder(input_shape, merging_technique, add_similarity):\r\n",
        "  input_claim = Input(shape=input_shape)\r\n",
        "  mask_claim = Masking(input_shape=input_shape, mask_value=0.0)(input_claim)\r\n",
        "  drop_claim = Dropout(0.2)(mask_claim)\r\n",
        "  lstm_claim = Bidirectional(LSTM(64, return_sequences=True, dropout=0.4))(drop_claim)\r\n",
        "  average_claim = GlobalAveragePooling1D()(lstm_claim)\r\n",
        "  \r\n",
        "  input_evidence = Input(shape=input_shape)\r\n",
        "  mask_evidence = Masking(input_shape=input_shape, mask_value=0.0)(input_evidence)\r\n",
        "  drop_evidence = Dropout(0.2)(mask_evidence)\r\n",
        "  lstm_evidence = Bidirectional(LSTM(64, return_sequences=True, dropout=0.4))(drop_evidence)\r\n",
        "  average_evidence = GlobalAveragePooling1D()(lstm_evidence)\r\n",
        "      \r\n",
        "  if merging_technique == 'concatenation':\r\n",
        "    merge = concatenate([average_claim, average_evidence])\r\n",
        "  elif merging_technique == 'sum':\r\n",
        "    merge = Add()([average_claim, average_evidence])\r\n",
        "  elif merging_technique == 'average':\r\n",
        "    merge = Average()([average_claim, average_evidence])\r\n",
        "\r\n",
        "  if add_similarity:\r\n",
        "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([average_claim, average_evidence])\r\n",
        "    merge = concatenate([merge, distance])\r\n",
        "\r\n",
        "  classificator = Dense(512, activation='relu')(merge)\r\n",
        "  classificator = Dropout(0.2)(classificator)\r\n",
        "  classificator = Dense(1, activation='sigmoid')(classificator)\r\n",
        "  \r\n",
        "  opt = Adam(learning_rate=0.001)\r\n",
        "  \r\n",
        "  model = Model(inputs=[input_claim, input_evidence], outputs=classificator)\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_OGNSKoPI9Z"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtYQaNYPHtv"
      },
      "source": [
        "def MLP_encoder(input_shape, merging_technique, add_similarity):\r\n",
        "  input_claim = Input(shape=input_shape)\r\n",
        "  mask_claim = Masking(input_shape=input_shape, mask_value=0.0)(input_claim)\r\n",
        "  drop_claim = Dropout(0.2)(mask_claim)\r\n",
        "  reshape_claim = Reshape((input_shape[0] * input_shape[1],))(drop_claim)\r\n",
        "  mlp_claim = Dense(512)(reshape_claim)\r\n",
        "\r\n",
        "  input_evidence = Input(shape=input_shape)\r\n",
        "  mask_evidence = Masking(input_shape=input_shape, mask_value=0.0)(input_evidence)\r\n",
        "  drop_evidence = Dropout(0.2)(mask_evidence)\r\n",
        "  reshape_evidence = Reshape((input_shape[0] * input_shape[1],))(drop_evidence)\r\n",
        "  mlp_evidence = Dense(512)(reshape_evidence)\r\n",
        "  \r\n",
        "  if merging_technique == 'concatenation':\r\n",
        "    merge = concatenate([mlp_claim, mlp_evidence])\r\n",
        "  elif merging_technique == 'sum':\r\n",
        "    merge = Add()([mlp_claim, mlp_evidence])\r\n",
        "  elif merging_technique == 'average':\r\n",
        "    merge = Average()([mlp_claim, mlp_evidence])\r\n",
        "\r\n",
        "  if add_similarity:\r\n",
        "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([lstm_claim, lstm_evidence])\r\n",
        "    merge = concatenate([merge, distance])\r\n",
        "\r\n",
        "  classificator = Dense(512, activation='relu')(merge)\r\n",
        "  classificator = Dropout(0.2)(classificator)\r\n",
        "  classificator = Dense(1, activation='sigmoid')(classificator)\r\n",
        "  \r\n",
        "  opt = Adam(learning_rate=0.001)\r\n",
        "  \r\n",
        "  model = Model(inputs=[input_claim, input_evidence], outputs=classificator)\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWLG8jGVREOP"
      },
      "source": [
        "## BOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJowxqWiRDZL"
      },
      "source": [
        "def BOV(input_shape, merging_technique, add_similarity):\r\n",
        "  input_claim = Input(shape=input_shape)\r\n",
        "  drop_claim = Dropout(0.2)(input_claim)\r\n",
        "  average_claim = GlobalAveragePooling1D()(drop_claim)\r\n",
        "\r\n",
        "  input_evidence = Input(shape=input_shape)\r\n",
        "  drop_evidence = Dropout(0.2)(input_evidence)\r\n",
        "  average_evidence = GlobalAveragePooling1D()(drop_evidence)\r\n",
        "\r\n",
        "  if merging_technique == 'concatenation':\r\n",
        "    merge = concatenate([average_claim, average_evidence])\r\n",
        "  elif merging_technique == 'sum':\r\n",
        "    merge = Add()([average_claim, average_evidence])\r\n",
        "  elif merging_technique == 'average':\r\n",
        "    merge = Average()([average_claim, average_evidence])\r\n",
        "\r\n",
        "  if add_similarity:\r\n",
        "    distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([lstm_claim, lstm_evidence])\r\n",
        "    merge = concatenate([merge, distance])\r\n",
        "  \r\n",
        "  classificator = Dense(512, activation='relu')(merge)\r\n",
        "  classificator = Dropout(0.2)(classificator)\r\n",
        "  classificator = Dense(1, activation='sigmoid')(classificator)\r\n",
        "  \r\n",
        "  opt = Adam(learning_rate=0.001)\r\n",
        "  \r\n",
        "  model = Model(inputs=[input_claim, input_evidence], outputs=classificator)\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fKwSyV5J3u"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKN8lDLPYxKI"
      },
      "source": [
        "def train(encoder, merging_technique, add_similarity):\r\n",
        "  \r\n",
        "  model = encoder((MAX_SENTENCE_LENGTH, embedding_dimension), merging_technique, add_similarity)\r\n",
        "  \r\n",
        "  n_epochs = 30\r\n",
        "  steps_per_epoch = len(train_df) / n_epochs\r\n",
        "  validation_steps = len(val_df) / n_epochs\r\n",
        "  cb = EarlyStopping(monitor='val_accuracy', patience=n_epochs, restore_best_weights=True)\r\n",
        "\r\n",
        "  train_generator = dataframe_generator(train_df)\r\n",
        "  validation_generator = dataframe_generator(val_df)\r\n",
        "  \r\n",
        "  model.fit(train_generator, epochs=n_epochs, steps_per_epoch=steps_per_epoch,\r\n",
        "            validation_data=validation_generator, validation_steps=validation_steps,\r\n",
        "            callbacks=cb)\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_3z5NrYP3l9"
      },
      "source": [
        "encoders = {\r\n",
        "            'simpleLSTM': simple_LSTM_encoder,\r\n",
        "            'averageLSTM': average_LSTM_encoder,\r\n",
        "            'MLP': MLP_encoder,\r\n",
        "            'BOV': BOV\r\n",
        "}\r\n",
        "\r\n",
        "merging_techniques = ['concatenation', \r\n",
        "                      'sum',\r\n",
        "                      'average'\r\n",
        "                      ]\r\n",
        "\r\n",
        "\r\n",
        "y_test = test_df['Embedded label']\r\n",
        "\r\n",
        "results = []\r\n",
        "\r\n",
        "for encoder in encoders:\r\n",
        "  print(\"=====================================================================================================================================\")\r\n",
        "  print(\"Embedding with\", encoder)\r\n",
        "\r\n",
        "  \r\n",
        "  print(\"Merging using\", 'average')\r\n",
        "  classificator = train(encoders[encoder], 'average', add_similarity=False)\r\n",
        "    \r\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------\")\r\n",
        "    \r\n",
        "  test_generator = dataframe_generator(test_df)\r\n",
        "  y_pred = np.around(classificator.predict(test_generator))\r\n",
        "    \r\n",
        "  results.append([encoder, 'average', classification_report(y_test, y_pred, zero_division=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3KjJSch3Cf8"
      },
      "source": [
        "# evaluate best encoder using only average strategy\r\n",
        "\r\n",
        "for report in results:\r\n",
        "  print(\"Encoder:\", report[0])\r\n",
        "  print(\"Merging technique:\", report[1])\r\n",
        "  print(report[2])\r\n",
        "  print(\"------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUS7YhX92GMN"
      },
      "source": [
        "print(\"=====================================================================================================================================\")\r\n",
        "print(\"Embedding with averageLSTM\")\r\n",
        "\r\n",
        "results_averageLSTM = []\r\n",
        "\r\n",
        "for technique in merging_techniques:\r\n",
        "    print(\"Merging using\", technique)\r\n",
        "    classificator = train(average_LSTM_encoder, technique, add_similarity=False)\r\n",
        "    \r\n",
        "    print(\"--------------------------------------------------------------------------------------------------------------------------------------\")\r\n",
        "\r\n",
        "    test_generator = dataframe_generator(test_df)\r\n",
        "    \r\n",
        "    y_pred = np.around(classificator.predict(test_generator))\r\n",
        "\r\n",
        "    results_averageLSTM.append(['averageLSTM', technique, classification_report(y_test, y_pred, zero_division=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQoTR_p3Ngl"
      },
      "source": [
        "# evaluate best technique fot best encoder\r\n",
        "\r\n",
        "for report in results_averageLSTM:\r\n",
        "  print(\"Encoder:\", report[0])\r\n",
        "  print(\"Merging technique:\", report[1])\r\n",
        "  print(report[2])\r\n",
        "  print(\"------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igyPoEyQ2aW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89f9d35-c78a-4e2f-bfeb-d0a4412403c9"
      },
      "source": [
        "BEST_ENCODER = average_LSTM_encoder\r\n",
        "BEST_TECHNIQUE = 'average'\r\n",
        "\r\n",
        "print(\"Embedding with average LSTM\")\r\n",
        "print(\"Merging using\", BEST_TECHNIQUE)\r\n",
        "print(\"Adding cosine similarity\")\r\n",
        "classificator = train(BEST_ENCODER, BEST_TECHNIQUE, add_similarity=True)\r\n",
        "test_generator = dataframe_generator(test_df)\r\n",
        "y_pred = np.around(classificator.predict(test_generator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding with average LSTM\n",
            "Merging using average\n",
            "Adding cosine similarity\n",
            "Epoch 1/30\n",
            "4058/4058 [==============================] - 102s 21ms/step - loss: 0.4156 - accuracy: 0.8627 - val_loss: 0.8860 - val_accuracy: 0.5858\n",
            "Epoch 2/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.3716 - accuracy: 0.8671 - val_loss: 0.9384 - val_accuracy: 0.6067\n",
            "Epoch 3/30\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.3974 - accuracy: 0.8594 - val_loss: 0.7765 - val_accuracy: 0.6444\n",
            "Epoch 4/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.4168 - accuracy: 0.8515 - val_loss: 0.5893 - val_accuracy: 0.6987\n",
            "Epoch 5/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.4093 - accuracy: 0.8412 - val_loss: 0.7482 - val_accuracy: 0.6402\n",
            "Epoch 6/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.3983 - accuracy: 0.8482 - val_loss: 0.5963 - val_accuracy: 0.7113\n",
            "Epoch 7/30\n",
            "4058/4058 [==============================] - 81s 20ms/step - loss: 0.4616 - accuracy: 0.8159 - val_loss: 0.6045 - val_accuracy: 0.7280\n",
            "Epoch 8/30\n",
            "4058/4058 [==============================] - 80s 20ms/step - loss: 0.4425 - accuracy: 0.8200 - val_loss: 0.5424 - val_accuracy: 0.7238\n",
            "Epoch 9/30\n",
            "4058/4058 [==============================] - 80s 20ms/step - loss: 0.4316 - accuracy: 0.8286 - val_loss: 0.7165 - val_accuracy: 0.6820\n",
            "Epoch 10/30\n",
            "4058/4058 [==============================] - 80s 20ms/step - loss: 0.4498 - accuracy: 0.8118 - val_loss: 0.5795 - val_accuracy: 0.6653\n",
            "Epoch 11/30\n",
            "4058/4058 [==============================] - 80s 20ms/step - loss: 0.4819 - accuracy: 0.7971 - val_loss: 0.6293 - val_accuracy: 0.7113\n",
            "Epoch 12/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.4294 - accuracy: 0.8218 - val_loss: 0.6457 - val_accuracy: 0.6025\n",
            "Epoch 13/30\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.4731 - accuracy: 0.7735 - val_loss: 0.5461 - val_accuracy: 0.7071\n",
            "Epoch 14/30\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.4661 - accuracy: 0.7997 - val_loss: 0.4798 - val_accuracy: 0.7615\n",
            "Epoch 15/30\n",
            "4058/4058 [==============================] - 79s 20ms/step - loss: 0.4714 - accuracy: 0.7906 - val_loss: 0.6123 - val_accuracy: 0.6820\n",
            "Epoch 16/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.4808 - accuracy: 0.7858 - val_loss: 0.5248 - val_accuracy: 0.7113\n",
            "Epoch 17/30\n",
            "4058/4058 [==============================] - 80s 20ms/step - loss: 0.4489 - accuracy: 0.7923 - val_loss: 0.4874 - val_accuracy: 0.7238\n",
            "Epoch 18/30\n",
            "4058/4058 [==============================] - 80s 20ms/step - loss: 0.4539 - accuracy: 0.8020 - val_loss: 0.5792 - val_accuracy: 0.7364\n",
            "Epoch 19/30\n",
            "4058/4058 [==============================] - 81s 20ms/step - loss: 0.4545 - accuracy: 0.7954 - val_loss: 0.5584 - val_accuracy: 0.6987\n",
            "Epoch 20/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.4284 - accuracy: 0.8142 - val_loss: 0.5905 - val_accuracy: 0.7029\n",
            "Epoch 21/30\n",
            "4058/4058 [==============================] - 78s 19ms/step - loss: 0.4714 - accuracy: 0.7873 - val_loss: 0.5515 - val_accuracy: 0.7280\n",
            "Epoch 22/30\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.3928 - accuracy: 0.8320 - val_loss: 0.5737 - val_accuracy: 0.6778\n",
            "Epoch 23/30\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.3964 - accuracy: 0.8335 - val_loss: 0.4897 - val_accuracy: 0.7322\n",
            "Epoch 24/30\n",
            "4058/4058 [==============================] - 79s 20ms/step - loss: 0.3692 - accuracy: 0.8493 - val_loss: 0.5247 - val_accuracy: 0.7448\n",
            "Epoch 25/30\n",
            "4058/4058 [==============================] - 79s 20ms/step - loss: 0.3931 - accuracy: 0.8356 - val_loss: 0.7866 - val_accuracy: 0.6695\n",
            "Epoch 26/30\n",
            "4058/4058 [==============================] - 79s 20ms/step - loss: 0.3737 - accuracy: 0.8630 - val_loss: 0.6235 - val_accuracy: 0.6904\n",
            "Epoch 27/30\n",
            "4058/4058 [==============================] - 81s 20ms/step - loss: 0.3431 - accuracy: 0.8688 - val_loss: 0.5281 - val_accuracy: 0.7155\n",
            "Epoch 28/30\n",
            "4058/4058 [==============================] - 81s 20ms/step - loss: 0.3730 - accuracy: 0.8515 - val_loss: 0.6464 - val_accuracy: 0.6402\n",
            "Epoch 29/30\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.3200 - accuracy: 0.8668 - val_loss: 0.5197 - val_accuracy: 0.7280\n",
            "Epoch 30/30\n",
            "4058/4058 [==============================] - ETA: 0s - loss: 0.3315 - accuracy: 0.8683WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 238.83333333333334 batches). You may need to use the repeat() function when building your dataset.\n",
            "4058/4058 [==============================] - 79s 19ms/step - loss: 0.3315 - accuracy: 0.8684 - val_loss: 0.5578 - val_accuracy: 0.7220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yYWYwFk5SP2"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ybj8CLO-JiI"
      },
      "source": [
        "## Multi input classification evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frl8oR9RQ77o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bab593-f5dd-4e43-a7b1-eb09321d5123"
      },
      "source": [
        "y_test = test_df['Embedded label']\r\n",
        "print(classification_report(y_test, y_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.48      0.62      3583\n",
            "           1       0.64      0.92      0.76      3606\n",
            "\n",
            "    accuracy                           0.70      7189\n",
            "   macro avg       0.75      0.70      0.69      7189\n",
            "weighted avg       0.75      0.70      0.69      7189\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ4ZQoe1-Mci"
      },
      "source": [
        "## Claim verification evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TrXBwa-O9tj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c79e096-f3a8-4fca-f22c-6554dadd7f09"
      },
      "source": [
        "from scipy import stats\r\n",
        "\r\n",
        "# Add prediction column to df\r\n",
        "test_df['Prediction'] = y_pred.astype(int)\r\n",
        "# collect predictions for each claim in a list\r\n",
        "voting_df = test_df.groupby(['ID', 'Embedded label'])['Prediction'].apply(list).reset_index()\r\n",
        "# find the mode in each prediction list\r\n",
        "voting_df['Majority label'] = voting_df['Prediction'].apply(lambda x: stats.mode(x)[0][0])\r\n",
        "\r\n",
        "y_grouped_test = voting_df['Embedded label']\r\n",
        "y_majority_pred = voting_df['Majority label']\r\n",
        "\r\n",
        "print(classification_report(y_grouped_test, y_majority_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.49      0.62      3304\n",
            "           1       0.64      0.92      0.76      3309\n",
            "\n",
            "    accuracy                           0.70      6613\n",
            "   macro avg       0.75      0.70      0.69      6613\n",
            "weighted avg       0.75      0.70      0.69      6613\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzesA23o5Z7T"
      },
      "source": [
        "# Comments/summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKw3VIY95h4o"
      },
      "source": [
        "### Dataset preprocessing\r\n",
        "We cleaned a bit the texts removing parentheses, punctuation and strange characters.\r\n",
        "\r\n",
        "### Dataset conversion\r\n",
        "We embedded each word of claims and evidences using glove embedding and we treated OOV using the neighbour strategy.\r\n",
        "\r\n",
        "### Model definition\r\n",
        "We defined 4 different models:\r\n",
        "\r\n",
        "\r\n",
        "*   A bidirectional LSTM which takes as input a sentence and return the last hidden state as sentence encoding\r\n",
        "*   A bidirectional LSTM which takes as input a sentence and return the average of all the hidden states as sentence encoding\r\n",
        "*   A simple MultiLayer Perceptron which takes as input a sentence and returns the output of the fully connected layer as the encoding of the sentence \r\n",
        "*   A BOV model which simply takes as input a sentence and returns the average of the glove embedding of all the words in the sentence\r\n",
        "\r\n",
        "### Training\r\n",
        "In the training, first all the models are tested using the same merging strategy: average. \r\n",
        "\r\n",
        "Then the best model is chosen and is tested with all the merging strategies.\r\n",
        "\r\n",
        "In the end the best model is tested with the best strategy and with the cosine similarity concatenated to the input of the classifier.\r\n",
        "\r\n",
        "The notebook we uploaded only shows results from the last run, since it was too long to run everything together.\r\n",
        "\r\n",
        "### Evaluation\r\n",
        "First we evaluated the results of the classifier looking only at the numbers of the pairs claim/evidence correctly classified.\r\n",
        "\r\n",
        "Then we aggregated the pairs referring to the same claim and through a major voting we evaluated the quality of the classification in relation the claim.\r\n"
      ]
    }
  ]
}